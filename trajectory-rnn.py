# -*- coding: utf-8 -*-
"""main_2.ipynb

Automatically generated by Colab.

"""



import os
import json
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
from pathlib import Path
import torch.optim as optim
import logging
from sklearn.preprocessing import StandardScaler
from torch.optim.lr_scheduler import StepLR
import matplotlib.pyplot as plt
import gdown
import zipfile
import torch
import numpy as np
import random
# Set seed for reproducibility
seed = 42
torch.manual_seed(seed)
np.random.seed(seed)
random.seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Function to download and extract dataset from Google Drive
def download_dataset():
    # Google Drive file ID
    file_id = "1zAY7OMcn62WvoE9RjsFhJUr-a5q-za-x"
    output_dir = "C:/Users/YourUsername/Datasets/objects_assignment"
    os.makedirs(output_dir, exist_ok=True)

    # Download the dataset
    url = f"https://drive.google.com/uc?id={file_id}"
    output_zip = os.path.join(output_dir, "objects_assignment.zip")
    gdown.download(url, output_zip, quiet=False)

    # Extract the dataset
    with zipfile.ZipFile(output_zip, 'r') as zip_ref:
        zip_ref.extractall(output_dir)
    os.remove(output_zip)  # Delete the zip file after extraction

    return output_dir

# Utility function to find valid sequences
def find_sequences(data_dir):
    data_dir = Path(data_dir)
    images_dir = data_dir / "images"
    if not images_dir.exists():
        logging.error(f"Error: Directory '{images_dir}' does not exist!")
        return []

    sequence_names = sorted([seq.name for seq in images_dir.iterdir() if seq.is_dir()])
    valid_sequences = []

    for seq in sequence_names:
        if all((data_dir / folder / seq).exists() for folder in ["images", "waypoints", "objects"]):
            valid_sequences.append(seq)
        else:
            logging.warning(f"Skipping sequence {seq} due to missing files.")

    return valid_sequences

# Dataset Class
class DrivingDataset(Dataset):
    def __init__(self, data_dir: str, sequence_names: list):
        self.data_dir = Path(data_dir)
        self.sequence_names = sequence_names
        self.samples = []
        self.object_scaler = StandardScaler()
        self.lane_scaler = StandardScaler()
        self.imu_scaler = StandardScaler()
        self._prepare_data()

    def _prepare_data(self):
        object_data, lane_data, imu_data = [], [], []

        # Collect data for normalization
        for sequence in self.sequence_names:
            imu_path = self.data_dir / 'objects' / sequence / 'imu_data.json'
            objects_path = self.data_dir / 'objects' / sequence / 'cametra_interface_output.csv'
            lanes_path = self.data_dir / 'objects' / sequence / 'cametra_interface_lanes_output.csv'
            waypoints_path = self.data_dir / 'waypoints' / sequence / 'waypoints.npy'

            if not all(p.exists() for p in [imu_path, objects_path, lanes_path, waypoints_path]):
                logging.warning(f"Skipping sequence {sequence} due to missing files.")
                continue

            try:
                with open(imu_path, 'r') as f:
                    imu_data_json = json.load(f)

                objects_df = pd.read_csv(objects_path)
                lanes_df = pd.read_csv(lanes_path)
                waypoints = np.load(waypoints_path)

                for timestamp, imu in imu_data_json.items():
                    frame_objects = objects_df[objects_df['name'] == timestamp]
                    frame_lanes = lanes_df[lanes_df['frame_id'] == timestamp]

                    object_features = frame_objects[['lat_dist', 'long_dist', 'abs_vel_x', 'abs_vel_z']].values
                    lane_features = frame_lanes[['polynomial[0]', 'polynomial[1]', 'polynomial[2]']].values
                    imu_features = np.array([imu['vf']]).reshape(-1, 1)  # Reshape to (1, 1)

                    if object_features.shape[0] == 0:
                        object_features = np.zeros((1, 4))
                    if lane_features.shape[0] == 0:
                        lane_features = np.zeros((1, 3))

                    object_data.append(object_features)
                    lane_data.append(lane_features)
                    imu_data.append(imu_features)
            except Exception as e:
                logging.error(f"Error processing sequence {sequence}: {e}")

        # Fit scalers
        self.object_scaler.fit(np.vstack(object_data))
        self.lane_scaler.fit(np.vstack(lane_data))
        self.imu_scaler.fit(np.vstack(imu_data))

        # Prepare samples
        for sequence in self.sequence_names:
            imu_path = self.data_dir / 'objects' / sequence / 'imu_data.json'
            objects_path = self.data_dir / 'objects' / sequence / 'cametra_interface_output.csv'
            lanes_path = self.data_dir / 'objects' / sequence / 'cametra_interface_lanes_output.csv'
            waypoints_path = self.data_dir / 'waypoints' / sequence / 'waypoints.npy'

            if not all(p.exists() for p in [imu_path, objects_path, lanes_path, waypoints_path]):
                continue

            try:
                with open(imu_path, 'r') as f:
                    imu_data_json = json.load(f)

                objects_df = pd.read_csv(objects_path)
                lanes_df = pd.read_csv(lanes_path)
                waypoints = np.load(waypoints_path)

                for idx, (timestamp, imu) in enumerate(imu_data_json.items()):
                    frame_objects = objects_df[objects_df['name'] == timestamp]
                    frame_lanes = lanes_df[lanes_df['frame_id'] == timestamp]

                    object_features = frame_objects[['lat_dist', 'long_dist', 'abs_vel_x', 'abs_vel_z']].values
                    lane_features = frame_lanes[['polynomial[0]', 'polynomial[1]', 'polynomial[2]']].values
                    imu_features = np.array([imu['vf']]).reshape(-1, 1)  # Reshape to (1, 1)

                    if object_features.shape[0] == 0:
                        object_features = np.zeros((1, 4))
                    if lane_features.shape[0] == 0:
                        lane_features = np.zeros((1, 3))

                    # Normalize features
                    object_features = self.object_scaler.transform(object_features)
                    lane_features = self.lane_scaler.transform(lane_features)
                    imu_features = self.imu_scaler.transform(imu_features)

                    self.samples.append({
                        'timestamp': timestamp,
                        'sequence': sequence,
                        'object_features': object_features,
                        'lane_features': lane_features,
                        'imu_features': imu_features,
                        'waypoints': waypoints[idx]
                    })
            except Exception as e:
                logging.error(f"Error processing sequence {sequence}: {e}")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]
        return {
            'object_features': torch.FloatTensor(sample['object_features']),
            'lane_features': torch.FloatTensor(sample['lane_features']),
            'imu_features': torch.FloatTensor(sample['imu_features']),
            'object_mask': torch.ones(len(sample['object_features'])),
            'waypoints': torch.FloatTensor(sample['waypoints'])
        }

# Model Class
class WaypointPredictor(nn.Module):
    def __init__(self, object_dim=4, lane_dim=3, imu_dim=1, hidden_dim=256):
        super(WaypointPredictor, self).__init__()
        self.object_encoder = nn.Sequential(
            nn.Linear(object_dim, hidden_dim),
            nn.ELU(),  # Use ELU instead of LeakyReLU
            nn.LayerNorm(hidden_dim)
        )
        self.lane_encoder = nn.Sequential(
            nn.Linear(lane_dim, hidden_dim),
            nn.ELU(),
            nn.LayerNorm(hidden_dim)
        )
        self.imu_encoder = nn.Sequential(
            nn.Linear(imu_dim, hidden_dim),
            nn.ELU(),
            nn.LayerNorm(hidden_dim)
        )
        self.object_attention = nn.MultiheadAttention(hidden_dim, 4, batch_first=True)
        self.fusion_layer = nn.Sequential(
            nn.Linear(hidden_dim * 3, hidden_dim * 2),
            nn.ELU(),
            nn.Dropout(0.5),
            nn.LayerNorm(hidden_dim * 2),
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ELU(),
            nn.Dropout(0.5),
            nn.LayerNorm(hidden_dim)
        )
        self.rnn = nn.GRU(hidden_dim, hidden_dim, batch_first=True)
        self.waypoint_head = nn.Linear(hidden_dim, 2)

    def forward(self, object_features, lane_features, imu_features, object_mask):
        batch_size, seq_len, _ = object_features.shape

        if seq_len == 0:
            object_features = torch.zeros((batch_size, 1, 4), device=object_features.device)
            object_mask = torch.zeros((batch_size, 1), dtype=torch.bool, device=object_features.device)

        # Encode features
        encoded_objects = self.object_encoder(object_features)  # Shape: (batch_size, seq_len, hidden_dim)
        encoded_lanes = self.lane_encoder(lane_features).mean(dim=1)  # Shape: (batch_size, hidden_dim)
        encoded_imu = self.imu_encoder(imu_features).mean(dim=1)  # Shape: (batch_size, hidden_dim)

        # Apply attention
        object_mask = ~object_mask.bool()  # Invert mask for MultiheadAttention
        attended_objects, _ = self.object_attention(
            encoded_objects, encoded_objects, encoded_objects,
            key_padding_mask=object_mask
        )  # Shape: (batch_size, seq_len, hidden_dim)

        # Take the mean over the sequence length
        attended_objects = attended_objects.mean(dim=1)  # Shape: (batch_size, hidden_dim)

        # Combine features
        combined = torch.cat([attended_objects, encoded_lanes, encoded_imu], dim=-1)  # Shape: (batch_size, hidden_dim * 3)
        fused = self.fusion_layer(combined)  # Shape: (batch_size, hidden_dim)

        # RNN for waypoint prediction
        hidden = None
        waypoints = []
        rnn_input = fused.unsqueeze(1)  # Shape: (batch_size, 1, hidden_dim)
        for _ in range(4):
            output, hidden = self.rnn(rnn_input, hidden)  # Shape: (batch_size, 1, hidden_dim)
            waypoint = self.waypoint_head(output)  # Shape: (batch_size, 1, 2)
            waypoints.append(waypoint)
            rnn_input = output

        return torch.cat(waypoints, dim=1)  # Shape: (batch_size, 4, 2)

# Training function
def train_model(data_dir, num_epochs=50, learning_rate=0.0001):
    # Find valid sequences
    sequence_names = find_sequences(data_dir)

    if not sequence_names:
        logging.error("No valid sequences found. Exiting training.")
        return None

    # Initialize dataset
    dataset = DrivingDataset(data_dir, sequence_names)
    logging.info(f"Dataset size: {len(dataset)}")

    if len(dataset) == 0:
        logging.error("Dataset is empty. Exiting training.")
        return None

    # Split into training and validation sets
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size

    if train_size == 0 or val_size == 0:
        logging.error("Not enough data for training and validation. Exiting training.")
        return None

    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])

    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

    # Initialize model, loss function, and optimizer
    model = WaypointPredictor()
    criterion = nn.MSELoss()  # Use MSE instead of MAE
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)  # Add weight decay
    scheduler = StepLR(optimizer, step_size=5, gamma=0.1)  # Learning rate scheduler

    # Lists to store loss values
    train_losses = []
    val_losses = []

    # Training loop
    model.train()
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        batch_count = 0

        for batch in train_loader:
            batch_count += 1

            if batch['object_features'].size(0) == 0 or batch['waypoints'].size(0) == 0:
                logging.warning("Empty batch detected, skipping...")
                continue

            optimizer.zero_grad()
            outputs = model(
                batch['object_features'],
                batch['lane_features'],
                batch['imu_features'],
                batch['object_mask']
            )

            loss = criterion(outputs, batch['waypoints'])
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            epoch_loss += loss.item()

        epoch_loss /= batch_count if batch_count > 0 else 1
        train_losses.append(epoch_loss)
        logging.info(f"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.8f}")

        # Validation
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for batch in val_loader:
                outputs = model(
                    batch['object_features'],
                    batch['lane_features'],
                    batch['imu_features'],
                    batch['object_mask']
                )
                loss = criterion(outputs, batch['waypoints'])
                val_loss += loss.item()
        val_loss /= len(val_loader)
        val_losses.append(val_loss)
        logging.info(f"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.8f}")
        model.train()

        scheduler.step()  # Update learning rate

    # Save the trained model
    torch.save(model.state_dict(), 'waypoint_predictor.pth')
    logging.info("Model saved to 'waypoint_predictor.pth'.")

    # Plot training and validation loss
    plt.figure(figsize=(10, 5))
    plt.plot(train_losses, label='Training Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    plt.grid()
    plt.show()

    return model

# Main function
def main():
    # Download dataset and get the path
    data_dir = download_dataset()

    # Train the model
    trained_model = train_model(data_dir)
    logging.info("Model training completed.")

if __name__ == "__main__":
    main()

